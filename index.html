<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="We introduce the function encoder, a representation learning algorithm that represents
                                    functions as a linear combination of learned basis functions.
                                    This yields fully-informative, linear representations.
                                    By representing the context in a reinforcement learning setting, this algorithm allows
                                    basic reinforcement learning algorithms to achieve zero-shot transfer in multi-task,
                                    multi-agent, and hidden-parameter reinforcement learning.">
  <meta property="og:title" content="Zero-Shot Reinforcement Learning via Function Encoders"/>
  <meta property="og:description" content="We introduce the function encoder, a representation learning algorithm that represents
                                    functions as a linear combination of learned basis functions.
                                    This yields fully-informative, linear representations.
                                    By representing the context in a reinforcement learning setting, this algorithm allows
                                    basic reinforcement learning algorithms to achieve zero-shot transfer in multi-task,
                                    multi-agent, and hidden-parameter reinforcement learning."/>
  <meta property="og:url" content="https://tyler-ingebrand.github.io/FunctionEncoderRL/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/cover.png" />
  <meta property="og:image:width" content="2050"/>
  <meta property="og:image:height" content="650"/>


  <meta name="twitter:title" content="Zero-Shot Reinforcement Learning via Function Encoders">
  <meta name="twitter:description" content="We introduce the function encoder, a representation learning algorithm that represents
                                    functions as a linear combination of learned basis functions.
                                    This yields fully-informative, linear representations.
                                    By representing the context in a reinforcement learning setting, this algorithm allows
                                    basic reinforcement learning algorithms to achieve zero-shot transfer in multi-task,
                                    multi-agent, and hidden-parameter reinforcement learning.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/cover.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Reinforcement Learning, Basis Functions, Representation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Zero-Shot Reinforcement Learning via Function Encoders</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Zero-Shot Reinforcement Learning via Function Encoders</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://tyler-ingebrand.github.io/" target="_blank">Tyler Ingebrand</a>,</span>
                <span class="author-block">
                  <a href="https://amyzhang.github.io/" target="_blank">Amy Zhang</a>, and </span>
                  <span class="author-block">
                    <a href="https://www.ae.utexas.edu/people/faculty/faculty-directory/topcu" target="_blank">Ufuk Topcu</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">The University of Texas at Austin<br>ICML 2024</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2401.17173" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/tyler-ingebrand/FunctionEncoderRL/tree/main" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2401.17173" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/cover.png" alt="The function Encoder work flow. First, the space of perturbing functions,
      e.g. the space of reward functions or the space of adversary policies, is encoded into representations
      via the learned basis functions. Then, these representations are passed into the RL algorithm.  "
           class="teaser-image">
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Although reinforcement learning (RL) can solve
          many challenging sequential decision making
          problems, achieving zero-shot transfer across related tasks remains a challenge. The difficulty
          lies in finding a good representation for the current task so that the agent understands how it relates to previously seen tasks. To achieve zeroshot transfer, we introduce the function encoder,
          a representation learning algorithm which represents a function as a weighted combination of
          learned, non-linear basis functions. By using a
          function encoder to represent the reward function or the transition function, the agent has information on how the current task relates to previously seen tasks via a coherent vector representation. Thus, the agent is able to achieve transfer between related tasks at run time with no additional training. We demonstrate state-of-theart data efficiency, asymptotic performance, and
          training stability in three RL fields by augmenting basic RL algorithms with a function encoder
          task representation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Idea -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths"><!-- Your image here -->
        <h2 class="title is-3">The Function Encoder</h2>
        <div class="content has-text-justified">
          <p>
            The goal of this paper is to represent functions from an arbitrary function space,
            so that this representation can be used for downstream tasks such as reinforcement learning.
            Naturally, functions are well-represented
            by their coefficients with respect to a given basis. However, many practical function spaces are high-dimensional, and so not amenable to
            classic basis functions such as Fourier series. Therefore, we aim to find basis functions for arbitrary function spaces from data.
            We introduce the function encoder, a algorithm which learns basis functions from data using a neural network.

          </p>
          <img src="static/images/function_encoder.gif" width="100%"
               alt="A video of learned basis functions converging to span the space of quadratic functions."/>
          <p>
            This video demonstrates the function encoder algorithm applied to the space of quadratic functions.
            Initially, the basis functions are random, and the randomly sampled quadratic functions (bottom) are poorly approximated.
            However, as training progresses, the basis functions converge and this approximation becomes more and more accurate.
            Furthermore, we can use the same basis functions to extrapolate to out-of-distribution functions not seen during training.
            Due to the nature of basis functions, these out-of-distribution quadratics are still well represented as they lie within the span of the basis functions.
            By construction, a function's representation, i.e. its coefficients with respect to the basis functions, is fully informative and linear.
            This property makes function encoder representations great for downstream tasks such as reinforcement learning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->


<!-- Dynamics -->
<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Hidden-Parameter Dynamics Predictions</h2>
        <div class="content has-text-justified">
          <div class="has-text-centered"><!-- Centered image -->
            <img src="static/images/mujoco.png" width="70%"
               alt="A MuJoCo Half Cheetah with Variable limb sizes."/>
          </div>
          <p>
            To demonstrate the efficacy of this approach, we first show that the function encoder can learn complex function spaces.
            We consider a modified version of the MuJoCo Half Cheetah environment where the lengths of the limbs and the control authority are varied ceach episode.
            These hidden-parameters affect the system dynamics.
            The goal is to predict the dynamics given a small online dataset, but without direct knowledge of the hidden parameters.
          </p>
          <div class="has-text-centered"><!-- Centered image -->
            <img src="static/images/mujoco_results.png" width="80%" align="center"
               alt="A learning curve showing that the function encoder achieves better performance than a transformer."/>
          </div>
          <p>
            We compare the function encoder against a transformer, which can incorporate the online dataset as input to the encoder side of the transformer.
            The function encoder outperforms the transformer in terms of data efficiency and asymptotic performance.
            We additionally compare against an oracle, which has access to the hidden parameters as an additional input.
            Interestingly, a variant of the function encoder even outperforms the oracle, which suggests the function encoder's inductive bias is great for this type of transfer.
          </p>
          <img src="static/images/mujoco_qual.png" width="100%" align="center"
                alt="A heat map shows that the function encoder's dynamics representation is smooth with respect to a change in hidden parameters"/>
            <p>
              Each axis of this plot shows a change in hidden parameter. The colors represent the cosine similarity between the dynamics representations
              of two different hidden parameter values. The figure shows that the function encoder's representation is smooth with respect to a change in hidden parameters.
              This suggests this representation is easy to work with for downstream tasks.
            </p>

        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->




<!-- Multi-task -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Multi-Task Reinforcement Learning</h2>
        <div class="content has-text-justified">
          <div class="has-text-centered"><!-- Centered image -->
            <img src="static/images/pacman_video.gif" width="60%"
               alt="Ms. Pacman going to different goal locations."/>
          </div>
          <p>
            In multi-task RL, the reward function varies every episode.
            We consider a modified version of Ms. Pacman where the goal location changes every episode.
            However, the agent does not know the goal location, and only has a dataset of state,action,reward pairs. Thus,
            this setting is more general than goal-reaching tasks alone.
            We proceed by first learning basis functions to span the space of reward functions. Then, for each reward function,
            we compute its representation according to the basis. Lastly, we pass this representation into the RL algorithm, in this case DQN.
            Due to the linearity of the representation space, we are also able to make architecture choices to take advantage of the inductive bias,
            i.e. by using an architecture similar to successor features.
          </p>
          <div class="has-text-centered"><!-- Centered image -->
            <img src="static/images/pacman_quan.png" width="80%" align="center"
               alt="A learning curve showing that the function encoder achieves better performance than baselines."/>
          </div>
          <p>
            We compare the function encoder against multi-task RL baselines. The function encoder outperforms the other approaches
            in terms of asymptotic performance. Interestingly, transformers also perform well in this setting.
          </p>
          <img src="static/images/pacman_qual.png" width="100%" align="center"
                alt="A heat map shows that the function encoder's reward representation is smooth with respect to a change in goal location"/>
            <p>
              We compare the reward function representations of each goal location, relative to the goal in the top-left marked with a star.
              The colors represent the cosine similarity between the reward representations of two different goal locations.
              The figure shows that the function encoder's representation is smooth with respect to a change in goal location,
              whereas the same is not always true for representation learning algorithms. The transformer also learns
              a smooth representation space in this example, which likely explains why it performs so well.
            </p>

        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->


<!-- Multi-Agent -->
<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Multi-Agent Reinforcement Learning</h2>
        <div class="content has-text-justified">
          <div class="has-text-centered"><!-- Centered image -->
            <img src="static/images/tag_video.gif" width="60%" style='border:1px solid #000000'
               alt="Two-player tag. "/>
          </div>
          <p>
            In this setting, the environment is a two-player, zero-sum game. Each episode, an adversary is randomly sampled,
            and the goal is for the ego agent to exploit the adversary. The environment is tag, and the ego agent is the tagger.
            Each episode, we presume access to a dataset of state, adversary action pairs. The ego agent should use this
            data to identify the adversary's policy, and then should exploit that policy if possible. In the function encoder case,
            this data is used to compute the adversary's representation with respect to learned basis functions, and this representation
            is fed into the RL algorithm.
          </p>
          <div class="has-text-centered"><!-- Centered image -->
            <img src="static/images/tag_quan.png" width="80%" align="center"
               alt="A learning curve showing that the function encoder achieves better performance than baselines."/>
          </div>
          <p>
            We compare the function encoder against multi-agent baselines, and its achieves the best asymptotic performance.
            Interestingly, PPO alone is unstable in this environment. This is a result of the unstationary nature of multi-agent RL.
            In contrast, PPO + FE is stable and achieves good performance.
            In addition, the transformer performs poorly in this environment. This is due to the inherent data inefficiency of transformers, where data is much
            more restricted in multi-agent settings then it is in multi-task settings.
            Lastly, we observe that a one-hot encoding of the adversary is not sufficient to achieve good performance.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{FunctionEncoder,
  author       = {Tyler Ingebrand and
                  Amy Zhang and
                  Ufuk Topcu},
  title        = {Zero-Shot Reinforcement Learning via Function Encoders},
  booktitle    = {{ICML}},
  year         = {2024}
}
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
