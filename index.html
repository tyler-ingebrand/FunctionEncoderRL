<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="We introduce the function encoder, a representation learning algorithm that represents
                                    functions as a linear combination of learned basis functions.
                                    This yields fully-informative, linear representations.
                                    By representing the context in a reinforcement learning setting, this algorithm allows
                                    basic reinforcement learning algorithms to achieve zero-shot transfer in multi-task,
                                    multi-agent, and hidden-parameter reinforcement learning.">
  <meta property="og:title" content="Zero-Shot Reinforcement Learning via Function Encoders"/>
  <meta property="og:description" content="We introduce the function encoder, a representation learning algorithm that represents
                                    functions as a linear combination of learned basis functions.
                                    This yields fully-informative, linear representations.
                                    By representing the context in a reinforcement learning setting, this algorithm allows
                                    basic reinforcement learning algorithms to achieve zero-shot transfer in multi-task,
                                    multi-agent, and hidden-parameter reinforcement learning."/>
  <meta property="og:url" content="https://tyler-ingebrand.github.io/FunctionEncoderRL/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/cover.png" />
  <meta property="og:image:width" content="2050"/>
  <meta property="og:image:height" content="650"/>


  <meta name="twitter:title" content="Zero-Shot Reinforcement Learning via Function Encoders">
  <meta name="twitter:description" content="We introduce the function encoder, a representation learning algorithm that represents
                                    functions as a linear combination of learned basis functions.
                                    This yields fully-informative, linear representations.
                                    By representing the context in a reinforcement learning setting, this algorithm allows
                                    basic reinforcement learning algorithms to achieve zero-shot transfer in multi-task,
                                    multi-agent, and hidden-parameter reinforcement learning.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/cover.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Reinforcement Learning, Basis Functions, Representation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Zero-Shot Reinforcement Learning via Function Encoders</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Zero-Shot Reinforcement Learning via Function Encoders</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://tyler-ingebrand.github.io/" target="_blank">Tyler Ingebrand</a>,</span>
                <span class="author-block">
                  <a href="https://amyzhang.github.io/" target="_blank">Amy Zhang</a>, and </span>
                  <span class="author-block">
                    <a href="https://www.ae.utexas.edu/people/faculty/faculty-directory/topcu" target="_blank">Ufuk Topcu</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">The University of Texas at Austin<br>ICML 2024</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2401.17173" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/tyler-ingebrand/FunctionEncoderRL/tree/main" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2401.17173" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/cover.png" alt="The function Encoder work flow. First, the space of perturbing functions,
      e.g. the space of reward functions or the space of adversary policies, is encoded into representations
      via the learned basis functions. Then, these representations are passed into the RL algorithm.  "
           class="teaser-image">
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Although reinforcement learning (RL) can solve
          many challenging sequential decision making
          problems, achieving zero-shot transfer across related tasks remains a challenge. The difficulty
          lies in finding a good representation for the current task so that the agent understands how it relates to previously seen tasks. To achieve zeroshot transfer, we introduce the function encoder,
          a representation learning algorithm which represents a function as a weighted combination of
          learned, non-linear basis functions. By using a
          function encoder to represent the reward function or the transition function, the agent has information on how the current task relates to previously seen tasks via a coherent vector representation. Thus, the agent is able to achieve transfer between related tasks at run time with no additional training. We demonstrate state-of-theart data efficiency, asymptotic performance, and
          training stability in three RL fields by augmenting basic RL algorithms with a function encoder
          task representation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Idea -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths"><!-- Your image here -->
        <div class="content has-text-justified">
          <p>
            The goal of this paper is to represent functions from an arbitrary function space,
            so that this representation can be used for downstream tasks such as reinforcement learning.
            Naturally, functions are well-represented
            by their coefficients with respect to a given basis. However, many practical function spaces are high-dimensional, and so not amenable to
            classic basis functions such as Fourier series. Therefore, we aim to find basis functions for arbitrary function spaces from data.
            We introduce the function encoder, a algorithm which learns basis functions from data using a neural network.

          </p>
          <img src="static/images/function_encoder.gif" width="100%"
               alt="A video of learned basis functions converging to span the space of quadratic functions."/>
          <p>
            This video demonstrates the function encoder algorithm applied to the space of quadratic functions.
            Initially, the basis functions are random, and the randomly sampled quadratic functions (bottom) are poorly approximated.
            However, as training progresses, the basis functions converge and this approximation becomes more and more accurate.
            Furthermore, we can use the same basis functions to extrapolate to out-of-distribution functions not seen during training.
            Due to the nature of basis functions, these out-of-distribution quadratics are still well represented as they lie within the span of the basis functions.
            By construction, a function's representation, i.e. its coefficients with respect to the basis functions, is fully informative and linear.
            This property makes function encoder representations great for downstream tasks such as reinforcement learning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->


<!-- Dynamics -->
<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths"><!-- Your image here -->
        <div class="content has-text-justified">
          <img src="static/images/mujoco.png" width="70%"
               alt="A MuJoCo Half Cheetah with Variable limb sizes."/>
          <p>
            To demonstrate the efficacy of this approach, we first show that the function encoder can learn complex function spaces.
            We consider a modified version of the MuJoCo Half Cheetah environment where the lengths of the limbs and the control authority are varied ceach episode.
            These hidden-parameters affect the system dynamics.
            The goal is to predict the dynamics given a small online dataset, but without direct knowledge of the hidden parameters.
          </p>
          <img src="static/images/mujoco_results.png" width="80%"
               alt="A learning curve showing that the function encoder achieves better performance than a transformer."/>
          <p>
            We compare the function encoder against a transformer, which can incorporate the online dataset as input to the encoder side of the transformer.
            The function encoder outperforms the transformer in terms of data efficiency and asymptotic performance.
            We additionally compare against an oracle, which has access to the hidden parameters as an additional input.
            Interestingly, a variant of the function encoder even outperforms the oracle, which suggests the function encoder's inductive bias is great for this type of transfer.
          </p>
          <img src="static/images/mujoco_qual.png" width="100%"
                alt="A heat map shows that the function encoder's dynamics representation is smooth with respect to a change in hidden parameters"/>
            <p>
              Each axis of this plot shows a change in hidden parameter. The colors represent the cosine similarity between the dynamics representations
              of two different hidden parameter values. The figure shows that the function encoder's representation is smooth with respect to a change in hidden parameters.
              This suggests this representation is easy to work with for downstream tasks.
            </p>

        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->






<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
